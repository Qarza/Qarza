{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qarza/Qarza/blob/main/Python_MCP_Server_Skeleton_for_Gemini_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "import aiohttp # Required: pip install aiohttp\n",
        "\n",
        "# --- Configuration ---\n",
        "# Gemini API Key: Best practice is to use an environment variable.\n",
        "# In the Canvas environment, the API key might be injected or handled differently.\n",
        "# For local development, you might set it like: export GEMINI_API_KEY=\"YOUR_API_KEY\"\n",
        "# If no API key is provided, the Gemini API call will likely fail.\n",
        "# For models like gemini-2.0-flash, an API key might not be strictly needed if running in a managed environment\n",
        "# that provides it automatically. Otherwise, it's required.\n",
        "API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\") # Default to empty string\n",
        "\n",
        "# Gemini API Endpoint - using gemini-2.0-flash as an example\n",
        "# Adjust the model name (e.g., gemini-pro) as needed.\n",
        "GEMINI_API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={API_KEY}\"\n",
        "\n",
        "# Server Configuration\n",
        "HOST = '0.0.0.0'  # Listen on all available network interfaces\n",
        "PORT = 8888       # Port for the server to listen on\n",
        "\n",
        "# --- Gemini API Interaction ---\n",
        "async def get_gemini_response(session: aiohttp.ClientSession, user_prompt: str, chat_history: list = None) -> str:\n",
        "    \"\"\"\n",
        "    Asynchronously sends a prompt to the Gemini API and returns the response.\n",
        "\n",
        "    Args:\n",
        "        session: An aiohttp.ClientSession object for making HTTP requests.\n",
        "        user_prompt: The prompt text from the user.\n",
        "        chat_history: Optional list of previous messages for conversational context.\n",
        "                      Example: [{\"role\": \"user\", \"parts\": [{\"text\": \"Hello\"}]},\n",
        "                                {\"role\": \"model\", \"parts\": [{\"text\": \"Hi there!\"}]}]\n",
        "\n",
        "    Returns:\n",
        "        The text response from Gemini AI, or an error message.\n",
        "    \"\"\"\n",
        "    if not chat_history:\n",
        "        chat_history = []\n",
        "\n",
        "    # Add the current user prompt to the history\n",
        "    chat_history.append({\"role\": \"user\", \"parts\": [{\"text\": user_prompt}]})\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": chat_history\n",
        "        # You can add generationConfig here if needed, e.g.,\n",
        "        # \"generationConfig\": {\n",
        "        #   \"temperature\": 0.7,\n",
        "        #   \"maxOutputTokens\": 1000\n",
        "        # }\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    print(f\"Sending to Gemini: {json.dumps(payload, indent=2)}\") # Log the request payload\n",
        "\n",
        "    try:\n",
        "        async with session.post(GEMINI_API_URL, json=payload, headers=headers) as response:\n",
        "            response_data = await response.json()\n",
        "            print(f\"Received from Gemini: {json.dumps(response_data, indent=2)}\") # Log the response\n",
        "\n",
        "            if response.status == 200:\n",
        "                if response_data.get(\"candidates\") and \\\n",
        "                   response_data[\"candidates\"][0].get(\"content\") and \\\n",
        "                   response_data[\"candidates\"][0][\"content\"].get(\"parts\") and \\\n",
        "                   response_data[\"candidates\"][0][\"content\"][\"parts\"][0].get(\"text\"):\n",
        "\n",
        "                    gemini_text_response = response_data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "                    # Add Gemini's response to chat history for context in next turn\n",
        "                    chat_history.append({\"role\": \"model\", \"parts\": [{\"text\": gemini_text_response}]})\n",
        "                    return gemini_text_response\n",
        "                else:\n",
        "                    error_message = \"Error: Unexpected response structure from Gemini API.\"\n",
        "                    print(error_message)\n",
        "                    print(\"Full response:\", response_data)\n",
        "                    # chat_history.pop() # Remove user prompt if API call failed structurally\n",
        "                    return error_message\n",
        "            else:\n",
        "                error_message = f\"Error: Gemini API request failed with status {response.status}.\"\n",
        "                print(error_message)\n",
        "                print(\"Error details:\", response_data)\n",
        "                # chat_history.pop() # Remove user prompt if API call failed\n",
        "                return error_message\n",
        "    except aiohttp.ClientConnectorError as e:\n",
        "        error_message = f\"Error: Could not connect to Gemini API. {e}\"\n",
        "        print(error_message)\n",
        "        # chat_history.pop()\n",
        "        return error_message\n",
        "    except json.JSONDecodeError:\n",
        "        error_message = \"Error: Could not decode JSON response from Gemini API.\"\n",
        "        print(error_message)\n",
        "        # chat_history.pop()\n",
        "        return error_message\n",
        "    except Exception as e:\n",
        "        error_message = f\"An unexpected error occurred while calling Gemini API: {e}\"\n",
        "        print(error_message)\n",
        "        # chat_history.pop()\n",
        "        return error_message\n",
        "\n",
        "# --- Client Handling ---\n",
        "async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter):\n",
        "    \"\"\"\n",
        "    Handles a single client connection.\n",
        "    Receives messages, sends them to Gemini, and returns responses.\n",
        "    \"\"\"\n",
        "    client_address = writer.get_extra_info('peername')\n",
        "    print(f\"Accepted connection from {client_address}\")\n",
        "\n",
        "    # Initialize chat history for this client session\n",
        "    # This simple example keeps history in memory per connection.\n",
        "    # For persistence or shared history, you'd need a database or other storage.\n",
        "    client_chat_history = []\n",
        "\n",
        "    # Create a single aiohttp session for the duration of this client connection\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        try:\n",
        "            while True:\n",
        "                # 1. Read data from the client\n",
        "                #    We expect clients to send UTF-8 encoded messages followed by a newline.\n",
        "                data = await reader.readline()\n",
        "                if not data:\n",
        "                    print(f\"Client {client_address} disconnected (no data).\")\n",
        "                    break # Connection closed by client\n",
        "\n",
        "                message = data.decode('utf-8').strip()\n",
        "                if not message: # Ignore empty messages\n",
        "                    continue\n",
        "\n",
        "                print(f\"Received from {client_address}: {message}\")\n",
        "\n",
        "                if message.lower() == 'exit':\n",
        "                    print(f\"Client {client_address} requested exit.\")\n",
        "                    writer.write(\"Goodbye!\\n\".encode('utf-8'))\n",
        "                    await writer.drain()\n",
        "                    break\n",
        "\n",
        "                # 2. Get response from Gemini AI\n",
        "                #    The client_chat_history is passed to maintain conversation context.\n",
        "                #    It will be updated by get_gemini_response.\n",
        "                ai_response = await get_gemini_response(session, message, client_chat_history)\n",
        "\n",
        "                # 3. Send AI response back to the client\n",
        "                print(f\"Sending to {client_address}: {ai_response}\")\n",
        "                writer.write((ai_response + '\\n').encode('utf-8'))\n",
        "                await writer.drain()\n",
        "\n",
        "        except ConnectionResetError:\n",
        "            print(f\"Client {client_address} connection reset.\")\n",
        "        except asyncio.CancelledError:\n",
        "            print(f\"Client handler for {client_address} cancelled.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error handling client {client_address}: {e}\")\n",
        "            try:\n",
        "                # Try to inform the client about the error\n",
        "                error_msg = f\"Server error: {e}\\n\"\n",
        "                writer.write(error_msg.encode('utf-8'))\n",
        "                await writer.drain()\n",
        "            except Exception as ex_send:\n",
        "                print(f\"Could not send error to client {client_address}: {ex_send}\")\n",
        "        finally:\n",
        "            print(f\"Closing connection with {client_address}\")\n",
        "            writer.close()\n",
        "            await writer.wait_closed()\n",
        "\n",
        "# --- Main Server Logic ---\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Starts the TCP server.\n",
        "    \"\"\"\n",
        "    server = await asyncio.start_server(\n",
        "        handle_client,\n",
        "        HOST,\n",
        "        PORT\n",
        "    )\n",
        "\n",
        "    addr = server.sockets[0].getsockname()\n",
        "    print(f'Serving on {addr}')\n",
        "\n",
        "    async with server:\n",
        "        await server.serve_forever()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"Starting MCP Server...\")\n",
        "    print(f\"Gemini API Key Used: {'Yes' if API_KEY else 'No (Using default or environment-provided)'}\")\n",
        "    print(f\"Gemini API URL: {GEMINI_API_URL}\")\n",
        "    print(f\"Listening on {HOST}:{PORT}\")\n",
        "    print(\"Clients can connect via TCP (e.g., using netcat/telnet). Send 'exit' to close a connection.\")\n",
        "    print(\"Make sure you have 'aiohttp' installed: pip install aiohttp\")\n",
        "\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nServer shutting down...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to start server: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "w6bR4JIVq_qP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}